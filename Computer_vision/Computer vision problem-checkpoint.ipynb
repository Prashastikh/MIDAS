{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download train and test dataset from [here](https://drive.google.com/drive/folders/1F2PjpJ_u_iaD-Fs0wwcymRiVVLK34-Fu). This dataset has 4 classes. Labels for\n",
    "training data are provided, you have to submit labels of test data. Feel free to use any Machine\n",
    "learning or Deep learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used in the problem are: \n",
    "* [Numpy](http://www.numpy.org/) will be used for powerful matrix and scientific operations. \n",
    "* [Pandas](https://pandas.pydata.org/) for data transformation and analysis. \n",
    "* [Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/) for data visualization.\n",
    "* [Scikit-learn](https://scikit-learn.org/stable/) to use machine learning classifiers, splitting data and metrics for evaluation.\n",
    "* [Keras](https://keras.io/) to build deep learning based classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8cc73b448dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[1;31m#deep learning library\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m from keras.layers import (Conv2D, MaxPooling2D, \n",
      "\u001b[0;32mc:\\users\\prashasti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\prashasti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\prashasti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\prashasti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\prashasti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#standard utilities\n",
    "import os\n",
    "import pickle #to load pickle data\n",
    "from collections import Counter\n",
    "\n",
    "#data science and visualization libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, accuracy_score, \n",
    "                             confusion_matrix)  \n",
    "\n",
    "#deep learning library\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Conv2D, MaxPooling2D, \n",
    "                          Dense, Flatten, \n",
    "                          Dropout, BatchNormalization)\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main directory path for all the files\n",
    "PATH = r'..\\CV_problem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been provided in pickle format, hence, first step will be to load in the pickle file which can be easily done in 2 lines as shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{PATH}\\\\Data\\\\train_image.pkl', 'rb') as image_file:\n",
    "    train_images = pickle.load(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(train_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(np.load(f'{PATH}\\\\Data\\\\train_label.pkl', allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this dataset, we have 2000 training samples corresponding to each of the 4 classes: 0, 2, 3 and 6, hence, in total we have 8000 number of images for training. As all the classes are equally distributed, our dataset is perfectly balanced and doesn't need any kind of oversampling or undersampling.<br>\n",
    "Now, lets have look at some of the images from our training data. Before, plotting we will first convert the list: `train_images` to a `numpy` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2d03c494fd76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_images' is not defined"
     ]
    }
   ],
   "source": [
    "train_images = np.array(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of train_images: {train_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already now that 8000 is the number of training samples. We can conclude from shape of the `train_images` that each image has been represented by a vector of length 784. So, its very much likely that each of the images were initially of size: 28 X 28 pixels and have been flattened to 28\\*28 i.e. 784 length of vector. Let's see if our inference is correct by plotting the samples by reshaping them to a size of 28 X 28. Below is the function, `plot_multiple_data()` that will plot `n_rows*n_columns` number of images simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_data(n_rows, n_columns, indices):\n",
    "    '''\n",
    "    Parameters-\n",
    "        n_rows, n_columns: Number of rows and columns in the figure\n",
    "        indices: List of indices for the images from the dataset\n",
    "    '''\n",
    "    #figure that will be displayed\n",
    "    fig = plt.figure(figsize=(n_rows*2, n_columns*2))\n",
    "\n",
    "    #Showing first n_rows*n_columns images from the dataset specified by indices\n",
    "    for i in range(1, n_rows*n_columns + 1):\n",
    "        plt.subplot(n_rows, n_columns, i)\n",
    "        plt.imshow(train_images[indices[i]].reshape(28, 28))\n",
    "        plt.title(f'Label: {train_labels[indices[i]]}') #corresponding label to each of the image\n",
    "    fig.tight_layout()  #for better padding amongst subplots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.permutation(train_images.shape[0]) #generate random indices for plotting\n",
    "plot_multiple_data(10, 10, random_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the above subplots, we can say that:\n",
    "* Label 0 is for Half sleeve T-shirts/Tops\n",
    "* Label 2 is for Long sleeve T-shirts or Pullovers\n",
    "* Label 3 is for Dress\n",
    "* Label 6 is for Shirts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Pre-processing <a id='preprocessing'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting random seed so that every time we run random, we get the same result\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the available training data into train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here, we'll use `scikit-learn`'s `train_test_split` to split the data into 90:10 ratio. 90% data is for training and rest of the 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x, Val_x, Train_y, Val_y = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of Training features: {Train_x.shape}\")\n",
    "print(f\"Shape of Training labels: {Train_y.shape}\")\n",
    "print(f\"Shape of Validation features: {Val_x.shape}\")\n",
    "print(f\"Shape of Validation labels: {Val_y.shape}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing for CNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution Neural Networks (CNN) require images to be in 2D shape while in our case each image is a vector of 784 length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshaping the Train_x and Val_x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function, `reshape_vector` will take in the datasets with images in the vector form and will return datasets with reshaped representation of images. Each of the image will be represented by a matrix of shape, 28 x 28 x 1. Since the images are black and white, 3rd dimension is equal to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_vector(Train_x, Val_x):\n",
    "    '''\n",
    "    Parameters-\n",
    "        Train_x, Val_x: Training and validation sets with images as vector array\n",
    "    Returns-\n",
    "        Reshaped Train_x and Val_x \n",
    "    '''\n",
    "    return Train_x.reshape((-1, 28, 28, 1)), Val_x.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing by scaling down pixel values to the range [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Range of pixel values is 0 to 255. If we normalize the pixel values to a smaller range of 0 to 1, the model will be able to learn the real structures instead of dealing with the scale differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(Train_x, Val_x): \n",
    "    '''\n",
    "    Parameters-\n",
    "        Train_x, Val_x: Reshaped Train_x and Val_x\n",
    "    Returns-\n",
    "        Normalized Train_x and Val_x\n",
    "    \n",
    "    '''\n",
    "    return Train_x.astype(\"float32\") / 255.0, Val_x.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot encoding Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in the notebook, CNN classifier instead of directly outputting one of the classes, it will be outputting probabilities corresponding to each class. So, the output will always be in the range: [0, 1]. As, in our case, labels are 0, 2, 3 and 6, it is required to transform these labels and represent them in the form of 0 and 1. This transformation is nothing but one-hot encoding. <br>\n",
    "Here `pandas`' `get_dummies` method will come in handy that will easily transorm the data in required encoded form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Train_y, Val_y):\n",
    "    '''\n",
    "    Parameters-\n",
    "        Train_y, Val_y = Array of labels\n",
    "    Returns-\n",
    "        One-hot encoded labels\n",
    "    '''\n",
    "    return pd.get_dummies(Train_y), pd.get_dummies(Val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we are done with all the pre-processing, we can start trying machine learning algorithms and choose the one which outperforms all of the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. In scikit-learn's implementation, number of nearest neighbors is assigned by parameter, `n_neighbors`. Here, we'll use `n_neighbors` = 3. <br>\n",
    "Training data can be easily trained using this classifier by using `fit()` method on classifier's instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(Train_x, Train_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_predictions = neigh.predict(Val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_accuracy = accuracy_score(Val_y, neigh_predictions)\n",
    "print(f'Accuracy score of KNN Classifier: {neigh_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78% accuracy is reasonably good considering simplicity of algorithm and minimal parameter tuning. Let's look at how many samples from validation set were correctly predicted by plotting the confusion matrix. Again, we can easily get the values for confusion matrix using `scikit-learn` and it can then be plotted using `matplotlib` and `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    '''\n",
    "    Parameters-\n",
    "        y_true: Array of true labels\n",
    "        y_pred: Array of predicted labels\n",
    "        labels: List of labels ([0, 2, 3, 6])\n",
    "    '''\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot = True, ax = ax, fmt = 'g'); #annot=True to annotate cells, fmt='g' to sho\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = [0, 2, 3, 6]\n",
    "plot_confusion_matrix(Val_y, neigh_predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following conclusions are made:\n",
    "* Out of 217 samples that have label 0, 185 have been predicted correctly. \n",
    "* There are 25 samples whose actual label is 0, but were predicted as 6.\n",
    "* Similarly 30 samples with actual label as 2 are predicted 6.\n",
    "* Out of 208 samples that have label 6, 42 are predicted as 0, 33 are predicted as 2 and 7 are predicted 3.\n",
    "So, the model is mostly getting confused due to label 6. Let's look at some more metrics by generating classification report to get much clearer view at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neigh_classification_report = classification_report(Val_y, neigh_predictions)\n",
    "print(f'Classification report of KNN Classifier: \\n{neigh_classification_report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, precision, recall and f1-score is lowest when it comes to class label 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second classifier that we'll be using is Random Forest Classifier. This classifier is an ensemble of multiple decision trees that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.<br>\n",
    "In below cell, we've defined three parameters and rest are kept as default. `criterion` is the function to measure the quality of a split. Here it is set to `entropy` is for information gain that is computed using logarithmics. `max_depth` is the maximum depth for the decision trees in the forest. `n_estimators` is the number of trees to be used, usually the more, the better.<br>\n",
    "Same steps that were followed for KNN classifier will be followed here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(criterion='entropy', max_depth=50, n_estimators=100)\n",
    "forest.fit(Train_x, Train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_predictions = forest.predict(Val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_accuracy = accuracy_score(Val_y, forest_predictions)\n",
    "print(f'Accuracy score of Random Forest Classifier: {forest_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest classifier is certainly more accurate than KNN classifier. Let's see if it is able to correctly predict label 6 this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 2, 3, 6]\n",
    "plot_confusion_matrix(Val_y, forest_predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this time too classifier performs most poorly for label 6, but there certainly is significant improvement, if we look at number of correctly predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_classification_report = classification_report(Val_y, forest_predictions)\n",
    "print(f'Classification report of Random Forest Classifier: \\n{forest_classification_report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to CNNs, that are meant to outperform other algorithms when it comes to computer vision problems, we'll give a shot to one more popular classification algorithm i.e. Support Vector Machine Classifier. The idea of SVM classifier can be simply put as: The algorithm creates a line or a hyperplane which separates the data into classes.<br>\n",
    "Here, `scikit-learn`'s implementation SVC will be use for building the classifier. Specified parameters are: `C` which is a penalty parameter `C` of the error term, `kernel` is set to `poly` as the data is unstructured. `gamma` is the coefficient for kernel set to `auto` hence coefficient will be `1\\n_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(C=10, kernel='poly', gamma='auto')\n",
    "svc.fit(Train_x, Train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_predictions = svc.predict(Val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_accuracy = accuracy_score(Val_y, svc_predictions)\n",
    "print(f'Accuracy score of Support Vector Classifier: {svc_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are down on accuracy by about 1%. Let's look at confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 2, 3, 6]\n",
    "plot_confusion_matrix(Val_y, svc_predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classification_report = classification_report(Val_y, svc_predictions)\n",
    "print(f'Classification report of Support Vector Classifier: \\n{svc_classification_report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC performs equally good as Random forest classifier (some what poorer). At this point, we'vepretty much tried most used machine learning algorithms and should move on to much more complex, deep learning algorithm, CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs are made up of neurons with learnable weights and biases. \n",
    "Main advantage of CNNs over ANNs and other machine learning algorithms is that these operate over volumes. That is also the reason, we'll have to reshape the image vectors into 2D forms. <br>\n",
    "Now, we'll call the pre-processing functions that were exclusively defined for CNNs earlier in the [Data Pre-processing](#preprocessing) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x, Val_x = reshape_vector(Train_x, Val_x)\n",
    "Train_x, Val_x = normalize(Train_x, Val_x)\n",
    "Train_y, Val_y = one_hot_encode(Train_y, Val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above, we're all set to build the CNN model. Here, we'll use `Keras`' [Sequential API](https://keras.io/models/sequential/). Using this, model can be considered as a linear stack of layers, added one after other in sequential manner.<br> We'll keep tuning the parameters until the result recieved is reasonable. One thing, that is most important to account for is Overfitting, hence, we'll monitor the validation accuracy of the models at each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First CNN model will be consisting of following layers:\n",
    "* [Conv2D](https://keras.io/layers/convolutional/)\n",
    "* [MaxPooling2D](https://keras.io/layers/pooling/)\n",
    "* [Dropout](https://keras.io/layers/core/)\n",
    "* [Flatten](https://keras.io/layers/core/)\n",
    "* [Dense](https://keras.io/layers/core/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#feature extraction part\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), input_shape = (28,28,1), activation = 'relu')) #output: 26 x 26 x 32 \n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu')) #output: 24 x 24 x 32\n",
    "model.add(MaxPooling2D(pool_size = (2, 2))) #output: 12 x 12 x 32\n",
    "\n",
    "model.add(Dropout(0.50)) #output: 12 x 12 x 32\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu')) #output: 10 x 10 x 64\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu')) #output: 8 x 8 x 64\n",
    "model.add(MaxPooling2D(pool_size = (2, 2))) #output: 4 x 4 x 64\n",
    "\n",
    "model.add(Dropout(0.50)) #output: 4 x 4 x 64\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu')) #output: 2 x 2 x 32\n",
    "model.add(MaxPooling2D(pool_size = (2, 2))) #output: 1 x 1 x 32\n",
    "\n",
    "#classification part\n",
    "model.add(Flatten()) #output: 32\n",
    "model.add(Dense(units = 32, activation = \"relu\")) #output: 32\n",
    "model.add(Dense(units = 4, activation = \"softmax\")) #output: 4 (probabilites for each class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary for whole model by calling `summary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in total we have 84, 644 parameters to be trained. For training these parameters, we'll have to specify a loss function, an optimizer for that loss function and metrics to evaluate the model during compilation step.<br> As we are using softmax function to get the outputs, we'll have to choose a loss function that will increase the probability for true classes. This can be done in best way using cross entropy loss function. Now, to minimize this loss function there are several optimizers available. Here, we'll use Adam optimizer. The name \"Adam\" is derived from adaptive moment estimation and have the following advantages as coined by its authors: \n",
    "* Straightforward to implement and computationally efficient.\n",
    "* Invariant to diagonal rescale of the gradients.\n",
    "* Well suited for problems that are large in terms of data and/or parameters.\n",
    "* Appropriate for non-stationary objectives.\n",
    "* Appropriate for problems with very noisy/or sparse gradients.\n",
    "* Hyper-parameters have intuitive interpretation and typically require little tuning.<br>\n",
    "\n",
    "Learning rate `lr` of the Adam is set to 0.001 which is also the default value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer=Adam(lr=0.001), metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25 #number of times training data will pass through model\n",
    "batch_size=64 #number of training samples passed through model at a time\n",
    "history = model.fit(Train_x, Train_y,\n",
    "                     batch_size = batch_size,\n",
    "                     epochs = epochs,\n",
    "                     verbose = 2,\n",
    "                     validation_data = (Val_x, Val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got straight 4% increase in the validation accuracy by using CNN. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "train_acc = history.history['acc']\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history, n_epochs):\n",
    "    \n",
    "    '''\n",
    "    Parameters-\n",
    "    history: Default Keras' callback which records training metrics\n",
    "    n_epochs: Number of times data is passed through model during training\n",
    "    '''\n",
    "    \n",
    "    #list to keep accuracy and loss values obtained during training and validation\n",
    "    history_record = []\n",
    "    \n",
    "    #during training\n",
    "    history_record.append(history.history['loss'])\n",
    "    history_record.append(history.history['acc'])\n",
    "    \n",
    "    #during validation\n",
    "    history_record.append(history.history['val_loss'])\n",
    "    history_record.append(history.history['val_acc'])\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    #plotting two subplots\n",
    "    #first subplot is for loss values\n",
    "    #second for accuracy values\n",
    "    for i in range(1, 3):\n",
    "        \n",
    "        plt.subplot(1, 2, i)\n",
    "        plt.plot(np.arange(n_epochs), history_record[i - 1], label = \"Training\")\n",
    "        plt.plot(np.arange(n_epochs), history_record[i + 1], label = \"Validation\")\n",
    "        \n",
    "        #axis labels\n",
    "        plt.xlabel('Epochs')\n",
    "        if(i % 2 != 0):\n",
    "            plt.ylabel('Loss function values')\n",
    "        else:\n",
    "            plt.ylabel('Accuracy values')\n",
    "        plt.legend()\n",
    "        \n",
    "    fig.tight_layout() #for better padding amongst subplots\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(history, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see the gradual decrease in the loss and increase in accuracy values for training as well as validation sets. Also, the difference between training accuracy and validation accuracy is ~1%, hence, our model is clearly not overfitting.<br>\n",
    "\n",
    "Let's generate predictions and evaluate this model with metrics that we also used above for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(Val_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, a particular prediction will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(Val_x[0].reshape(1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output of the model is supposed to be softmaxed vector of length = number of classes. Above predicted vector is nothing but the probabilitiy corresponding to each class. As value at the 2nd position or index 1 is maximum, hence, predicted class is the second class i.e. 2. <br>\n",
    "\n",
    "In following cell, these predicted vectors are converted into corresponding class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary mapping from indices of output vector to corresponding class labels\n",
    "prediction_dict = {0: 0, 1: 2, 2: 3, 3: 6}\n",
    "\n",
    "#array to hold the predicted labels\n",
    "CNN_predictions = np.zeros(len(predictions))\n",
    "\n",
    "#loop through predicted vectors and adding predicted classes to CNN_predictions \n",
    "for i in range(len(predictions)):\n",
    "    \n",
    "    #get index of maximum element in vector\n",
    "    arg_max = np.argmax(predictions[i])\n",
    "    \n",
    "    #add value of key: arg_max from prediction_dict to CNN_predictions\n",
    "    CNN_predictions[i] = prediction_dict[arg_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `Val_y` has also been converted to an array of one hot vectors. We'll have to decode them to original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get index of maximum element in vector\n",
    "Orig_Val_y = Val_y.values.argmax(axis=1)\n",
    "\n",
    "Orig_Val_y = np.array([prediction_dict[y] for y in Orig_Val_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orig_Val_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `Orig_Val_y` as well as `CNN_predictions` are in the similar required form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = [0, 2, 3, 6]\n",
    "plot_confusion_matrix(Orig_Val_y, CNN_predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this model is performing  better than Random forest classifier and SVC but it is still underperforming for class label 6.<br>\n",
    "\n",
    "We'll save this model in HDF5 format for now so that if the later models does not outperform this one, we can consider this model as the final one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{PATH}//Models//model-1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model is now downloaded to local computer, it can be deleted from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of this second model will be kept almost same as the first one. Two notable changes are that this time `padding` parameter will be kept as `same` and number of output units for intermediate dense layer is now increased to 64 which will increase the number of parameters to 91, 972 as shown in the summary of the model. Increasing the number of parameters will increase model's complexity hence should be able to perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), input_shape = (28,28,1), padding = \"same\", activation = 'relu')) #output: 28 x 28 x 32 \n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = \"same\", activation = 'relu')) #output: 28 x 28 x 32\n",
    "model.add(MaxPooling2D(pool_size = (2, 2))) #output: 14 x 14 x 32\n",
    "\n",
    "model.add(Dropout(0.50)) #output: 14 x 14 x 32\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = \"same\", activation = 'relu')) #output: 14 x 14 x 64\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = \"same\", activation = 'relu')) #output: 14 x 14 x 64\n",
    "model.add(MaxPooling2D(pool_size = (2, 2))) #output: 7 x 7 x 64\n",
    "\n",
    "model.add(Dropout(0.50)) #output: 7 x 7 x 64\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu')) #output: 5 x 5 x 32\n",
    "model.add(MaxPooling2D(pool_size = (2, 2))) #output: 2 x 2 x 32\n",
    "\n",
    "model.add(Flatten()) #output: 128\n",
    "model.add(Dense(units = 64, activation = \"relu\")) #output: 64\n",
    "model.add(Dense(units = 4, activation = \"softmax\")) #output: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer= Adam(lr=0.001), metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40 #number of times training data will pass through model\n",
    "batch_size=64 #number of training samples passed through model at a time\n",
    "\n",
    "#Image augmenter\n",
    "aug = ImageDataGenerator(rotation_range=20, zoom_range=(0.9, 1.1),\n",
    "width_shift_range=0.1, height_shift_range=0.1, shear_range=0.5,\n",
    "horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "#fitting the model\n",
    "history = model.fit_generator(aug.flow(Train_x, Train_y, batch_size = batch_size),\n",
    "validation_data=(Val_x, Val_y), epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final validation accuracy for this model is 87.12% which is better than the previous model. Let's just repeat the steps we performed for generating predictions and confusion matrix for the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(Val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary mapping from indices of output vector to corresponding class labels\n",
    "prediction_dict = {0: 0, 1: 2, 2: 3, 3: 6}\n",
    "\n",
    "#array to hold the predicted labels\n",
    "CNN_predictions = np.zeros(len(predictions))\n",
    "\n",
    "#loop through predicted vectors and adding predicted classes to CNN_predictions \n",
    "for i in range(len(predictions)):\n",
    "    \n",
    "    #get index of maximum element in vector\n",
    "    arg_max = np.argmax(predictions[i])\n",
    "    \n",
    "    #add value of key: arg_max from prediction_dict to CNN_predictions\n",
    "    CNN_predictions[i] = prediction_dict[arg_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orig_Val_y = Val_y.values.argmax(axis=1)\n",
    "Orig_Val_y = np.array([prediction_dict[y] for y in Orig_Val_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = [0, 2, 3, 6]\n",
    "plot_confusion_matrix(Orig_Val_y, CNN_predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification report for CNN classifier: \\n{classification_report(Orig_Val_y, CNN_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has clearly outperformed all previous classifiers by good margin. Particularly, if we compare the classification report for class label 6, it has increased by almost 10%. <br>\n",
    "\n",
    "Let's now save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{PATH}//Models//model-2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 has shown good level of performance\n",
    "So, let's train the Model 2 on training data for 10 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Train_x, Train_y, batch_size = batch_size,\n",
    "validation_data=(Val_x, Val_y), epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we have an increase in accuracy of about 1% and 447 seconds that took it to train this model didn't go in vain. We'll call this model as Model 2.2 as it is just more trained version of Model 2. Let's again copy the above cells for prediction and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(Val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary mapping from indices of output vector to corresponding class labels\n",
    "prediction_dict = {0: 0, 1: 2, 2: 3, 3: 6}\n",
    "\n",
    "#array to hold the predicted labels\n",
    "CNN_predictions = np.zeros(len(predictions))\n",
    "\n",
    "#loop through predicted vectors and adding predicted classes to CNN_predictions \n",
    "for i in range(len(predictions)):\n",
    "    \n",
    "    #get index of maximum element in vector\n",
    "    arg_max = np.argmax(predictions[i])\n",
    "    \n",
    "    #add value of key: arg_max from prediction_dict to CNN_predictions\n",
    "    CNN_predictions[i] = prediction_dict[arg_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orig_Val_y = Val_y.values.argmax(axis=1)\n",
    "Orig_Val_y = np.array([prediction_dict[y] for y in Orig_Val_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = [0, 2, 3, 6]\n",
    "plot_confusion_matrix(Orig_Val_y, CNN_predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification report for CNN classifier: \\n{classification_report(Orig_Val_y, CNN_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{PATH}//Models//model-2.2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are done with building the classifiers and our final chosen model will be Model 2.2 as it has more generalization as well as prediction power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generating prediction for Test set using Model 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{PATH}//Data//test_image.pkl', 'rb') as image_file:\n",
    "    test_images = pickle.load(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of test samples: {len(test_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the images to numpy array\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of test images: {test_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the image vectors to 2D arrays\n",
    "test_images = test_images.reshape((-1, 28, 28, 1))\n",
    "print(f'Shape of test images: {test_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "test_images = test_images.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading trained model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f'{PATH}//Models//model-2.2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating predictions and converting them to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary mapping from indices of output vector to corresponding class labels\n",
    "prediction_dict = {0: 0, 1: 2, 2: 3, 3: 6}\n",
    "\n",
    "#array to hold the predicted labels for test set\n",
    "CNN_test_predictions = np.zeros(len(test_predictions))\n",
    "\n",
    "#loop through predicted vectors and adding predicted classes to CNN_predictions \n",
    "for i in range(len(test_predictions)):\n",
    "    \n",
    "    #get index of maximum element in vector\n",
    "    arg_max = np.argmax(test_predictions[i])\n",
    "    \n",
    "    #add value of key: arg_max from prediction_dict to CNN_predictions\n",
    "    CNN_test_predictions[i] = prediction_dict[arg_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_test_predictions = CNN_test_predictions.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the test images alongside there predicted labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading the test_images\n",
    "with open(f'{PATH}//Data//test_image.pkl', 'rb') as image_file:\n",
    "    test_images = np.array(pickle.load(image_file))\n",
    "    \n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "indices = np.random.permutation(test_images.shape[0]) #generating random indices for plotting\n",
    "\n",
    "#Showing first n_ images from the dataset specified by indices\n",
    "for i in range(1, 101):\n",
    "    plt.subplot(10, 10, i)\n",
    "    plt.imshow(test_images[indices[i]].reshape(28, 28))\n",
    "    plt.title(f'Predicted Label: {CNN_test_predictions[indices[i]]}') #corresponding predicted label to each of the image\n",
    "\n",
    "fig.tight_layout()  #for better padding amongst subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the prediction results in a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the required format of submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(f'{PATH}\\\\Data\\\\hitkul(sample_submission).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'image_index': np.arange(len(test_images)), 'class': CNN_test_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(f'{PATH}\\\\akshay_aggarwal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{PATH}\\\\akshay_aggarwal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
